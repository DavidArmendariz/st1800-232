{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/david/.local/share/virtualenvs/st1800-232-e4syQ74-/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/david/.local/share/virtualenvs/st1800-232-e4syQ74-/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# verificar que tengan instalado la librer√≠a 'pyspark', no requerido en AWS EMR/Spark\n",
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorios (path) de entrada y salida:\n",
    "# \n",
    "path_in=\"../datasets/\"\n",
    "path_out=\"../out/\"\n",
    "filenametxt_in='in.txt'\n",
    "filenametxt_out='out.txt'\n",
    "filenamecsv_in='in.csv'\n",
    "filenamecsv_out='out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session, no requerido en AWS EMR/Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([(1,'I really liked this movie'),\n",
    "                         (2,'I would recommend this movie to my friends'),\n",
    "                         (3,'movie was alright but acting was horrible'),\n",
    "                         (4,'I am never watching that movie ever again')],\n",
    "                        ['user_id','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+\n",
      "|user_id|review                                    |\n",
      "+-------+------------------------------------------+\n",
      "|1      |I really liked this movie                 |\n",
      "|2      |I would recommend this movie to my friends|\n",
      "|3      |movie was alright but acting was horrible |\n",
      "|4      |I am never watching that movie ever again |\n",
      "+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='review',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|user_id|review                                    |tokens                                             |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|1      |I really liked this movie                 |[i, really, liked, this, movie]                    |\n",
      "|2      |I would recommend this movie to my friends|[i, would, recommend, this, movie, to, my, friends]|\n",
      "|3      |movie was alright but acting was horrible |[movie, was, alright, but, acting, was, horrible]  |\n",
      "|4      |I am never watching that movie ever again |[i, am, never, watching, that, movie, ever, again] |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 21:09:29 WARN StopWordsRemover: Default locale set was [en_EC]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|user_id|tokens                                             |refined_tokens                    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|1      |[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
      "|2      |[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
      "|3      |[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
      "|4      |[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select(['user_id','tokens','refined_tokens']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=spark.read.csv('../datasets/movie_reviews.csv',inferSchema=True,header=True,sep=',')\n",
    "# datos desde S3\n",
    "# text_df=spark.read.csv('s3://bucket_name/datasets/movie_reviews.csv',inferSchema=True,header=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|              Review|Sentiment|              tokens|\n",
      "+--------------------+---------+--------------------+\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|\n",
      "|this was the firs...|        1|[this, was, the, ...|\n",
      "|i liked the Da Vi...|        1|[i, liked, the, d...|\n",
      "|i liked the Da Vi...|        1|[i, liked, the, d...|\n",
      "|I liked the Da Vi...|        1|[i, liked, the, d...|\n",
      "|that's not even a...|        1|[that's, not, eve...|\n",
      "|I loved the Da Vi...|        1|[i, loved, the, d...|\n",
      "|i thought da vinc...|        1|[i, thought, da, ...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|\n",
      "|I thought the Da ...|        1|[i, thought, the,...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|\n",
      "|then I turn on th...|        1|[then, i, turn, o...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|\n",
      "|i love da vinci c...|        1|[i, love, da, vin...|\n",
      "|i loved da vinci ...|        1|[i, loved, da, vi...|\n",
      "|TO NIGHT:: THE DA...|        1|[to, night::, the...|\n",
      "|THE DA VINCI CODE...|        1|[the, da, vinci, ...|\n",
      "|Thing is, I enjoy...|        1|[thing, is,, i, e...|\n",
      "|very da vinci cod...|        1|[very, da, vinci,...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 21:09:29 WARN StopWordsRemover: Default locale set was [en_EC]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_text_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|              Review|Sentiment|              tokens|      refined_tokens|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|this was the firs...|        1|[this, was, the, ...|[first, clive, cu...|\n",
      "|i liked the Da Vi...|        1|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|i liked the Da Vi...|        1|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|I liked the Da Vi...|        1|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|that's not even a...|        1|[that's, not, eve...|[even, exaggerati...|\n",
      "|I loved the Da Vi...|        1|[i, loved, the, d...|[loved, da, vinci...|\n",
      "|i thought da vinc...|        1|[i, thought, da, ...|[thought, da, vin...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|I thought the Da ...|        1|[i, thought, the,...|[thought, da, vin...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|then I turn on th...|        1|[then, i, turn, o...|[turn, light, rad...|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|i love da vinci c...|        1|[i, love, da, vin...|[love, da, vinci,...|\n",
      "|i loved da vinci ...|        1|[i, loved, da, vi...|[loved, da, vinci...|\n",
      "|TO NIGHT:: THE DA...|        1|[to, night::, the...|[night::, da, vin...|\n",
      "|THE DA VINCI CODE...|        1|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|Thing is, I enjoy...|        1|[thing, is,, i, e...|[thing, is,, enjo...|\n",
      "|very da vinci cod...|        1|[very, da, vinci,...|[da, vinci, code,...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import rand, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+-----------+\n",
      "|              Review|Sentiment|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+---------+--------------------+--------------------+-----------+\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|          4|\n",
      "|Da Vinci Code = U...|        0|[da, vinci, code,...|[da, vinci, code,...|         15|\n",
      "|I hate Harry Pott...|        0|[i, hate, harry, ...|[hate, harry, pot...|          7|\n",
      "|Oh, and Brokeback...|        0|[oh,, and, brokeb...|[oh,, brokeback, ...|          5|\n",
      "|Because I would l...|        1|[because, i, woul...|[like, make, frie...|          6|\n",
      "|I hate Harry Pott...|        0|[i, hate, harry, ...|[hate, harry, pot...|          8|\n",
      "|Ok brokeback moun...|        0|[ok, brokeback, m...|[ok, brokeback, m...|          5|\n",
      "|Da Vinci Code = U...|        0|[da, vinci, code,...|[da, vinci, code,...|         15|\n",
      "|gosh i miss telli...|        1|[gosh, i, miss, t...|[gosh, miss, tell...|          6|\n",
      "|The Da Vinci Code...|        1|[the, da, vinci, ...|[da, vinci, code,...|          7|\n",
      "+--------------------+---------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_text_df.coalesce(1\n",
    "              ).withColumn(\"tokens\", col(\"tokens\").cast(\"string\")\n",
    "              ).withColumn(\"refined_tokens\", col(\"refined_tokens\").cast(\"string\")\n",
    "              ).write.format(\"csv\"\n",
    "              ).option(\"header\", \"true\"\n",
    "              ).save(path_out+filenamecsv_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
